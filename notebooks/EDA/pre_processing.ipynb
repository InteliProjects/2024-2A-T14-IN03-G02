{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados contidos em datasets ou dataframes podem ser provenientes de diferentes fontes ou apresentar falhas e estar organizados de forma inadequada, resultando em análises falhas e modelos preditivos imprecisos. Sendo assim, para evitar que isso aconteça é preciso que os dados passem por uma etapa chamada de Pré-processamento. O Pré-processamento compreende um conjunto de atividades para preparação, organização e estruturação dos dados, de forma a corrigir inconsistências, agregar valor aos dados, e chegar a resultados de qualidade.\n",
    "\n",
    "Para entender essa etapa basta imaginar o processo de cozinhar uma refeição, que inicia pela preparação dos ingredientes, e envolve ações como lavar, cortar, medir e separar os itens antes de começar a cozinhar. Então, assim como a preparação dos ingredientes, a etapa de pré-processamento também envolve a limpeza dos dados, a remoção de valores desnecessários, e a organização dos dados em \"medidas\" corretas. Logo, para realizar um processamento e tratamento adequado dos dados, a etapa de pré-processamento se divide na limpeza, transformação e redução dos dados.\n",
    "\n",
    "A limpeza dos dados é responsável pelo preenchimento de dados ausentes, redução de ruídos, identificação e remoção de valores irregulares e pela resolução de inconsistências. Então, assim como você lava os vegetais, remove cascas e partes indesejadas antes de cozinhar, na limpeza de dados você remove valores irregulares, preenche dados ausentes e resolve inconsistências.\n",
    "\n",
    "Seguindo o exemplo, depois de lavar, você corta os ingredientes no tamanho certo, mede as quantidades e os organiza em tigelas separadas para facilitar o cozimento, assim como ocorre na transformação dos dados. Sendo essa etapa responsável pela normalização dos dados (garantindo que os dados estejam na mesma escala), seleção de atributos (escolhendo os ingredientes certos para a receita) e discretização (transformando ingredientes contínuos em porções específicas).\n",
    "\n",
    "Por fim, Se você tem muitos ingredientes ou quantidades excessivas, pode precisar diminuir a quantidade ou escolher os itens essenciais para garantir que o prato não fique sobrecarregado. Da mesma forma que ocorre na etapa de redução de dados, na qual você pode agregar informações, selecionar um subconjunto de atributos ou reduzir a dimensionalidade para tornar o processo mais eficiente.\n",
    "\n",
    "Sendo assim, para a etapa de pré-processamento foram criadas sete funções, que são responsáveis pelo tratamento de valores nulos, exclusão de colunas com valores nulos ou zeros, tratamento de outliers, e normalização dos valores. Além disso, este notebook está organizado intercalando as funções (código) e suas respectivas explicações (texto). As explicações contém o nome da função, sua descrição, os parâmetros, o retorno da função e o detalhamento do código, para consultar a utilização dessas funções e seus resultados no projeto consultar [notebook principal](./main.ipynb) ou a [documentação](../documents/documentacao.md) do projeto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importação das Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics as sts\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Funções de Pré-Processamento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Função:** pre_processing\n",
    "\n",
    "**Descrição:** Esta função é responsável por coordenar todo o processo de pré-processamento de um DataFrame. Ela aplica uma série de etapas para tratar valores nulos, remover colunas desnecessárias, tratar outliers e normalizar dados numéricos e categóricos.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- dataset: O DataFrame que será pré-processado.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- dataset: O DataFrame pré-processado.\n",
    "- columns_trate: Lista de colunas tratadas para valores nulos.\n",
    "- columns_drop_zero: Lista de colunas removidas por conterem apenas valores zero.\n",
    "- columns_drop_null: Lista de colunas removidas por conterem apenas valores nulos.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `trate_null_value(dataset):` Trata valores nulos nas colunas numéricas e categóricas preenchendo com a mediana ou moda, respectivamente.\n",
    "- `drop_columns_zero_values(dataset)`: Remove colunas que contêm apenas zeros.\n",
    "- `drop_columns_null_values(dataset):` Remove colunas que contêm apenas valores nulos.\n",
    "- `trate_outliers(dataset):` Substitui valores considerados outliers pela mediana da coluna.\n",
    "- `normalize_numerics_columns(dataset):` Normaliza todas as colunas numéricas.\n",
    "- `normalize_categoricals_columns(dataset):` Converte todas as colunas categóricas em valores numéricos utilizando codificação de rótulos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(dataset):\n",
    "    dataset, columns_trate = trate_null_value(dataset)  # Trata valores nulos nas colunas do DataFrame.\n",
    "    dataset, columns_drop_zero = drop_columns_zero_values(dataset)  # Remove colunas que contêm apenas valores zero.\n",
    "    dataset, columns_drop_null = drop_columns_null_values(dataset)  # Remove colunas que contêm apenas valores nulos.\n",
    "    dataset = trate_outliers(dataset)  # Substitui outliers nas colunas numéricas pela mediana.\n",
    "    dataset, label_encoders = normalize_categoricals_columns(dataset)  # Transforma colunas categóricas em valores numéricos.\n",
    "    dataset, scalers = normalize_numerics_columns(dataset)  # Normaliza colunas numéricas para média 0 e desvio padrão 1.\n",
    "    return dataset, columns_trate, columns_drop_zero, columns_drop_null, label_encoders, scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Função:** trate_null_value\n",
    "\n",
    "**Descrição**: Esta função identifica e trata valores nulos em um DataFrame preenchendo-os com a mediana para variáveis numéricas ou com a moda para variáveis categóricas.\n",
    "\n",
    "**Parâmetros**:\n",
    "\n",
    "- dataset: O DataFrame cujas colunas serão analisadas para valores nulos.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- dataset: O DataFrame com os valores nulos tratados.\n",
    "- columns_trate: Lista de colunas que foram tratadas devido à presença de valores nulos.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `dataset[column].isnull().sum() > 0`: Verifica se há valores nulos na coluna.\n",
    "- `np.issubdtype(dataset[column].dtype, np.number)`: Verifica se a coluna é do tipo numérico.\n",
    "- `dataset[column].dropna()`: Remove valores nulos para cálculo estatístico.\n",
    "- `sts.median(non_null_values)`: Calcula a mediana de valores não nulos.\n",
    "- `dataset[column].fillna(median, inplace=True)`: Preenche valores nulos com a mediana.\n",
    "- `dataset[column].mode()`: Calcula a moda para variáveis categóricas.\n",
    "- `dataset[column].fillna(mode[0], inplace=True)`: Preenche valores nulos com a moda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que trata valores nulos  \n",
    "def trate_null_value(dataset):\n",
    "    columns_trate = []\n",
    "    # Varre colunas do dataset recebido\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].isnull().sum() > 0:  # Verifica se há valores nulos na coluna\n",
    "            if np.issubdtype(dataset[column].dtype, np.number):  # Verifica se a coluna é numérica\n",
    "                non_null_values = dataset[column].dropna()  # Remove os NaN\n",
    "                if len(non_null_values) > 0:  # Verifica se há dados não nulos suficientes\n",
    "                    median = sts.median(non_null_values)  # Calcula a mediana\n",
    "                    dataset[column].fillna(median, inplace=True) # Preenche com a mediana todos os valores nulos\n",
    "                    columns_trate.append(f\"Coluna '{column}' foi tratada, pois continha valores nulos.\")\n",
    "            else:  # Para colunas categóricas\n",
    "                mode = dataset[column].mode() # Pega a moda da coluna\n",
    "                if not mode.empty:  # Verifica se há uma moda disponível\n",
    "                    dataset[column].fillna(mode[0], inplace=True)  # Preenche os NaN com a moda\n",
    "                    columns_trate.append(f\"Coluna '{column}' foi tratada, pois continha valores nulos.\")\n",
    "\n",
    "    return dataset, columns_trate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Função:** drop_columns_zero_values\n",
    "\n",
    "**Descrição**: Remove colunas que contêm apenas valores zero de um DataFrame, pois essas colunas não fornecem informações úteis.\n",
    "\n",
    "**Parâmetros**:\n",
    "\n",
    "- Dataset: O DataFrame cujas colunas serão analisadas para remoção.\n",
    "\n",
    "**Retorno**:\n",
    "\n",
    "- dataset: O DataFrame após a remoção das colunas.\n",
    "- columns_drop: Lista de colunas que foram removidas por conterem apenas zeros.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `if (dataset[column] == 0).all()`: Verifica se todos os valores de uma coluna são zero.\n",
    "- `dataset.drop(columns=[column], inplace=True)`: Remove a coluna se todos os valores forem zero.\n",
    "- `columns_drop.append(f\"Coluna '{column}'`: Adiciona em uma lista as colunas que foram removidas para log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para excluir colunas com valores que só contenham zeros\n",
    "def drop_columns_zero_values(dataset):\n",
    "    columns_drop = []\n",
    "    # Itera sobre todas as colunas do DataFrame\n",
    "    for column in dataset.columns:\n",
    "        # Verifica se todos os valores da coluna são iguais a zero\n",
    "        if (dataset[column] == 0).all():\n",
    "            # Remove a coluna se todos os valores forem zero\n",
    "            dataset.drop(columns=[column], inplace=True)\n",
    "            # Adiciona em uma lista as colunas que foram removidas para log\n",
    "            columns_drop.append(f\"Coluna '{column}' foi removida, pois contém apenas zeros.\")\n",
    "\n",
    "    return dataset, columns_drop\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Função:** drop_columns_null_values\n",
    "**Descrição**: Remove colunas que contêm apenas valores nulos de um DataFrame, pois essas colunas não contribuem com dados úteis.\n",
    "\n",
    "**Parâmetros**:\n",
    "\n",
    "- dataset: O DataFrame cujas colunas serão analisadas para remoção.\n",
    "\n",
    "**Retorno**:\n",
    "\n",
    "- dataset: O DataFrame após a remoção das colunas.\n",
    "- columns_drop: Lista de colunas que foram removidas por conterem apenas valores nulos.\n",
    "\n",
    "**Detalhamento do Código**:\n",
    "\n",
    "- `dataset[column].isnull().sum() == len(dataset[column])`: Verifica se todos os valores de uma coluna são nulos.\n",
    "- `dataset.drop(columns=[column], inplace=True)`: Remove a coluna se todos os seus valores forem nulos.\n",
    "- `columns_drop.append(f\"Coluna '{column}'`: Adiciona em uma lista as colunas que foram removidas para log.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para excluir colunas que só tenham valores nulos\n",
    "def drop_columns_null_values(dataset):\n",
    "    columns_drop = []\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].isnull().sum() > 0:  # Verifica se há valores nulos na coluna\n",
    "            dataset.drop(columns=[column], inplace=True)  # Remove a coluna inteira\n",
    "            # Adiciona em uma lista as colunas que foram removidas para log\n",
    "            columns_drop.append(f\"Coluna '{column}' excluída porque está completamente vazia.\")\n",
    "    return dataset, columns_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Função**: trate_outliers\n",
    "\n",
    "**Descrição**: Identifica e trata outliers nas colunas numéricas substituindo-os pela mediana da coluna, garantindo que outliers extremos não distorçam análises futuras.\n",
    "\n",
    "**Parâmetros**:\n",
    "\n",
    "- dataset: O DataFrame cujas colunas numéricas serão analisadas para outliers.\n",
    "\n",
    "**Retorno**:\n",
    "\n",
    "- dataset: O DataFrame com outliers tratados.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `np.nanmedian(dataset[column])`: Calcula a mediana ignorando valores nulos.\n",
    "- `stats.zscore(dataset[column].dropna())`: Calcula os z-scores para identificar outliers.\n",
    "- `dataset.loc[np.abs(zscores) > limiar, column] = median`: Substitui valores outliers pela mediana.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trate_outliers(dataset):\n",
    "    for column in dataset.columns:\n",
    "        if np.issubdtype(dataset[column].dtype, np.number):  # Verifica se a coluna é numérica\n",
    "            # Calcula a mediana ignorando os NaNs\n",
    "            median = np.nanmedian(dataset[column])\n",
    "            \n",
    "            # Calcula os z-scores ignorando os NaNs\n",
    "            zscores = stats.zscore(dataset[column].dropna())\n",
    "            \n",
    "            # Define um limiar de z-score (por exemplo, 3)\n",
    "            limiar = 3\n",
    "            \n",
    "            # Reatribui valores maiores que o limiar para a mediana\n",
    "            dataset.loc[np.abs(zscores) > limiar, column] = median\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Função:** normalize_numerics_columns\n",
    "\n",
    "**Descrição:** Normaliza todas as colunas numéricas de um DataFrame utilizando o `StandardScaler`, padronizando os dados para uma média de 0 e desvio padrão de 1. Além de armazenar os objetos `StandardScaler` utilizados para cada coluna, permitindo que a normalização possa ser revertida posteriormente.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- `dataset`: O DataFrame cujas colunas numéricas serão normalizadas.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- `dataset`: O DataFrame com as colunas numéricas normalizadas.\n",
    "- `scalers`: Um dicionário que armazena os objetos `StandardScaler` usados para cada coluna numérica.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `dataset.select_dtypes(include=[np.number]).columns`: Seleciona todas as colunas numéricas do DataFrame.\n",
    "- `StandardScaler()`: Inicializa um objeto `StandardScaler`, que será utilizado para padronizar os dados.\n",
    "- `scaler.fit_transform(dataset[[coluna]])`: Aplica a normalização à coluna numérica, convertendo os valores para uma escala com média 0 e desvio padrão 1.\n",
    "- `scalers[coluna] = scaler`: Armazena o objeto `StandardScaler` correspondente à coluna processada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para normalizar colunas numéricas\n",
    "def normalize_numerics_columns(dataset):\n",
    "    scalers= {}\n",
    "    numerics_columns = dataset.select_dtypes(include=[np.number]).columns\n",
    "    for coluna in numerics_columns:\n",
    "        scaler = StandardScaler()\n",
    "        dataset[coluna] = scaler.fit_transform(dataset[[coluna]])\n",
    "        scalers[coluna] = scaler  # Salva o scaler para reverter depois\n",
    "    return dataset, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Função:** normalize_categoricals_columns\n",
    "\n",
    "**Descrição:** Converte todas as colunas categóricas de um DataFrame em valores numéricos utilizando o `LabelEncoder`, transformando categorias em números inteiros.Além de armazenar os objetos `LabelEncoder` usados para cada coluna categórica.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- `dataset`: O DataFrame cujas colunas categóricas serão normalizadas.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- `dataset`: O DataFrame com as colunas categóricas convertidas em valores numéricos.\n",
    "- `label_encoders`: Um dicionário que armazena os objetos `LabelEncoder` utilizados para cada coluna categórica.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `dataset.select_dtypes(include=['object']).columns`: Seleciona todas as colunas categóricas do DataFrame.\n",
    "- `LabelEncoder()`: Inicializa um objeto `LabelEncoder`, que será utilizado para transformar os dados categóricos em números inteiros.\n",
    "- `label_encoder.fit_transform(dataset[coluna])`: Aplica a transformação à coluna categórica, convertendo cada categoria em um valor numérico.\n",
    "- `label_encoders[coluna] = label_encoder`: Armazena o objeto `LabelEncoder` correspondente à coluna processada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para normalizar colunas categóricas\n",
    "def normalize_categoricals_columns(dataset):\n",
    "    label_encoders = {}\n",
    "    colunas_categoricas = dataset.select_dtypes(include=['object']).columns\n",
    "    for coluna in colunas_categoricas:\n",
    "        label_encoder = LabelEncoder()\n",
    "        dataset[coluna] = label_encoder.fit_transform(dataset[coluna])\n",
    "        label_encoders[coluna] = label_encoder  # Salva o LabelEncoder para reverter depois\n",
    "    return dataset, label_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Função:** reverse_categoricals_columns\n",
    "\n",
    "**Descrição:** Reverte a normalização de colunas categóricas em um DataFrame, convertendo os valores numéricos de volta para suas respectivas categorias originais usando o `LabelEncoder`.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- `dataset`: O DataFrame que contém as colunas categorizadas que foram normalizadas e precisam ser revertidas para seus valores originais.\n",
    "- `label_encoders`: Um dicionário de `LabelEncoder` onde as chaves são os nomes das colunas e os valores são os objetos `LabelEncoder` que foram usados para codificar as colunas originais.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- `dataset`: O DataFrame com as colunas categóricas restauradas para seus valores originais.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `label_encoders.items()`: Itera sobre o dicionário que mapeia as colunas categóricas aos seus respectivos `LabelEncoder`.\n",
    "- `if coluna in dataset.columns`: Verifica se a coluna categórica está presente no DataFrame.\n",
    "- `pd.api.types.is_integer_dtype(dataset[coluna])`: Verifica se os dados da coluna estão no formato inteiro, o que é necessário para a transformação reversa.\n",
    "- `dataset[coluna].astype(int)`: Converte os dados da coluna para inteiro caso não estejam no formato correto.\n",
    "- `label_encoder.inverse_transform(dataset[coluna])`: Reverte a transformação numérica de volta para os valores categóricos originais usando o `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função ajustada para reverter a normalização das colunas categóricas\n",
    "def reverse_categoricals_columns(dataset, label_encoders):\n",
    "    # Iterar sobre o dicionário de label_encoders e verificar se a coluna existe no dataset\n",
    "    for coluna, label_encoder in label_encoders.items():\n",
    "        if coluna in dataset.columns:\n",
    "            # Verificar se os dados estão no formato correto para serem transformados\n",
    "            if not pd.api.types.is_integer_dtype(dataset[coluna]):\n",
    "                # Se a coluna não estiver em formato inteiro, tente convertê-la\n",
    "                try:\n",
    "                    dataset[coluna] = dataset[coluna].astype(int)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"Não foi possível converter a coluna {coluna} para inteiro.\")\n",
    "            # Reverter a codificação\n",
    "            dataset[coluna] = label_encoder.inverse_transform(dataset[coluna])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Função:** reverse_numerics_columns\n",
    "\n",
    "**Descrição:** Reverte a normalização das colunas numéricas de um DataFrame, restaurando os valores normalizados para sua escala original, utilizando um dicionário de escaladores (`scalers`), que foram aplicados às colunas numéricas durante o processo de normalização.\n",
    "\n",
    "**Parâmetros:**\n",
    "\n",
    "- `dataset`: O DataFrame que contém as colunas numéricas que foram normalizadas e precisam ser revertidas para seus valores originais.\n",
    "- `scalers`: Um dicionário onde as chaves são os nomes das colunas numéricas e os valores são os objetos `Scaler` (como `StandardScaler` ou `MinMaxScaler`) que foram utilizados para normalizar as colunas originais.\n",
    "\n",
    "**Retorno:**\n",
    "\n",
    "- `dataset`: O DataFrame com as colunas numéricas restauradas para seus valores originais.\n",
    "\n",
    "**Detalhamento do Código:**\n",
    "\n",
    "- `scalers.items()`: Itera sobre o dicionário que mapeia as colunas numéricas aos seus respectivos objetos `Scaler`.\n",
    "- `scaler.inverse_transform(dataset[[coluna]])`: Aplica a função `inverse_transform` para reverter a normalização das colunas, restaurando seus valores à escala original.\n",
    "- `dataset[[coluna]]`: Seleciona a coluna específica a ser transformada, garantindo que os dados estejam em formato de array 2D para o método `inverse_transform` do `Scaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para reverter a normalização das colunas numéricas\n",
    "def reverse_numerics_columns(dataset, scalers):\n",
    "    for coluna, scaler in scalers.items():\n",
    "        dataset[coluna] = scaler.inverse_transform(dataset[[coluna]])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Conclusão\n",
    "\n",
    "Sendo assim, nesse notebook foi desenvolvido um conjunto de funções para facilitar o pré-processamento de dados. Essas funções têm como objetivo principal preparar os dados para análises mais profundas e para as modelagens preditivas, abordando o tratamento de valores nulos, a remoção de colunas irrelevantes, a normalização dos dados e o tratamento de outliers.\n",
    "\n",
    "As funções para tratamento de valores nulos foram criadas para identificar e substituir valores ausentes nas colunas numéricas e categóricas. Para variáveis numéricas, os valores nulos são preenchidos pela mediana, enquanto para variáveis categóricas, a substituição é feita pela moda, assegurando que os dados estejam completos. Quanto às funções de remoção de colunas, estas foram desenvolvidas para excluir colunas que contenham apenas valores nulos ou apenas zeros, pois tais colunas não agregam valor às análises do projeto.\n",
    "\n",
    "Somado a essas, as funções de normalização foram implementadas para garantir que os dados numéricos sejam padronizados, com média zero e desvio padrão de um, enquanto para as colunas categóricas, as funções transformam essas variáveis em valores numéricos, facilitando sua utilização em modelos quantitativos. Além disso, as funções de tratamento de outliers substituem valores extremos pela mediana da coluna, minimizando o impacto de valores atípicos nos resultados.\n",
    "\n",
    "Em resumo, essas funções de pré-processamento foram desenvolvidas para serem reutilizáveis em diferentes conjuntos de dados, proporcionando um processo de manipulação de dados mais eficiente e padronizado. Ao encapsular a lógica de pré-processamento em funções, torna-se mais fácil aplicar técnicas de preparação de dados aos diferentes dataframes do projeto.\n",
    "\n",
    "Por fim, para consultar as funções desenvolvidas para a exploração dos dados acesse o [notebook de exploração de dados](./data_exploration.ipynb), para visualizar a aplicação dessas funções no projeto acesse o [notebook principal](./main.ipynb), e para obter as análises dos resultados obtidos nessas etapas consulte a [documentação](../documents/documentacao.md).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
